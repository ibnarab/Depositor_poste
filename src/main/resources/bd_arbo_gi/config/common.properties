###########################  bdl.  ##########################################
# cles obligatoires imposees par le cadre
# doivent commencer par "bdl."
#############################################################################

# liste des cles contenant des valeurs a ne pas afficher par les fonctions de logging
bdl.redaction=

# niveau de log pour les fonctions de logging (debug, info, warn ou error)
bdl.logger.level=info


###########################  app.  ##########################################
# Cles libres contenant les paramètres applicatifs/fonctionnels
# Spécifique à chaque projet.
# Le nombre de clés et leurs valeurs sont totalement libres.
# Elles doivent commencer par "app."
#############################################################################

# Les jobs dans YARN
app.yarn.queue=${GLB_POOL_RESSOURCE}
app.yarn.name.prefixe=${CODE_APPLI}${SUJET}_${INSTANCE}_${GLB_MODULE}
app.yarn.name={{app.yarn.name.prefixe}}

# fichier a utiliser pour log4j
app.log4j.file=log4j_mixte.properties

#********************#
#     COMPACTAGE     #
#********************#
cpt.queue_name={{app.yarn.queue}}
cpt.principal={{spk.common.principal}}
cpt.keytab={{spk.common.keytab}}

####################################################################################################
# Retention HDFS
####################################################################################################
app.purge.hdfs.retention=15

#********************#
#     Rattrapage     #
#********************#
# Dates de debut et de fin de l interval a traiter, au format YYYY-MM-JJ (ex: 2019-06-25). Est au maximum egal a la date de la veille
app.first_date_to_process=
app.last_date_to_process=


# DATABASE et PATH HDFS du RAW du SUJET du PROJET
app.database.raw=${DATABASE_RAW}
app.hdfs.path.raw.to_validate=${HDFS_RAW}/${GLB_MODULE}/to_validate
app.hdfs.path.raw.rejected=${HDFS_RAW}/${GLB_MODULE}/rejected
app.hdfs.path.raw.to_process=${HDFS_RAW}/${GLB_MODULE}/to_process
app.hdfs.path.raw.failed=${HDFS_RAW}/${GLB_MODULE}/failed
app.hdfs.path.raw.compacted=${HDFS_RAW}/${GLB_MODULE}/compacted
app.hdfs.path.raw.processed=${HDFS_RAW}/${GLB_MODULE}/processed

# DATABASE et PATH HDFS du SAFE du SUJET du PROJET
app.database.safe=${DATABASE_SAFE}
app.hdfs.path.safe=${HDFS_SAFE}/${GLB_MODULE}

# DATABASE et PATH HDFS de l'OPTIMIZED du SUJET du PROJET
#app.database.optimized=${DATABASE_TRAITEMENT_OPTIMIZED}
app.database.optimized=bd_aa_traitement_optimized
#app.hdfs.path.optimized=${HDFS_TRAITEMENT_OPTIMIZED}
app.hdfs.path.optimized=/data/bd_/business_area_aa/traitement/optimized/s_trt_depot_admission

# Table RAW admission
app.table.raw.admission=r_${GLB_MODULE}_vt1_cdepositorraw_in_v1
app.table.raw.admission.temp=t_${GLB_MODULE}_vt1_cdepositorraw_in_v1
app.{{app.table.raw.admission}}.database={{app.database.raw}}
app.{{app.table.raw.admission}}.hdfs.path={{app.hdfs.path.raw.processed}}/{{app.table.raw.admission}}
app.{{app.table.raw.admission}}.hdfs.path.failed={{app.hdfs.path.raw.failed}}/{{app.table.raw.admission.temp}}

# Table RAW ajout
app.table.raw.ajout=t_${GLB_MODULE}_ajout_raw
app.{{app.table.raw.ajout}}.database={{app.database.raw}}
app.{{app.table.raw.ajout}}.hdfs.path={{app.hdfs.path.raw.to_process}}/{{app.table.raw.ajout}}
app.{{app.table.raw.ajout}}.hql.create=${INSTALL}/hql/PAGDATAIA-41/create_raw_ajout.hql
app.{{app.table.raw.ajout}}.file=Depot_Prod.csv


# Table SAFE bronze admission
app.table.safe.bronze.admission=b_${GLB_MODULE}_admission
app.{{app.table.safe.bronze.admission}}.database={{app.database.safe}}
app.{{app.table.safe.bronze.admission}}.hdfs.path={{app.hdfs.path.safe}}/{{app.table.safe.bronze.admission}}
app.{{app.table.safe.bronze.admission}}.format=parquet
app.{{app.table.safe.bronze.admission}}.hql.create=${INSTALL}/hql/create_admission.hql
app.{{app.table.safe.bronze.admission}}.spark.class=alimadmission
cpt.{{app.table.safe.bronze.admission}}.queue_name=${GLB_POOL_RESSOURCES}
cpt.{{app.table.safe.bronze.admission}}.job_name={{app.yarn.name.prefixe}}_{{app.table.safe.bronze.admission}}_compact
cpt.{{app.table.safe.bronze.admission}}.mode_compression=snappy
cpt.{{app.table.safe.bronze.admission}}.taille_bloc=125
cpt.{{app.table.safe.bronze.admission}}.nb_fichiers=1
cpt.{{app.table.safe.bronze.admission}}.nb_fichiers_etalon=1
cpt.{{app.table.safe.bronze.admission}}.sparkparam.executor-cores=2
cpt.{{app.table.safe.bronze.admission}}.sparkparam.executor-memory=4G


# Table SAFE temporaire ajout
app.table.safe.bronze.ajout=t_${GLB_MODULE}_ajout_safe
app.{{app.table.safe.bronze.ajout}}.database={{app.database.safe}}
app.{{app.table.safe.bronze.ajout}}.hdfs.path={{app.hdfs.path.safe}}/{{app.table.safe.bronze.ajout}}
app.{{app.table.safe.bronze.ajout}}.format=parquet
app.{{app.table.safe.bronze.ajout}}.hql.create=${INSTALL}/hql/PAGDATAIA-41/create_safe_ajout.hql

# Table OPTIMIZED silver admission
app.table.optimized.silver.admission=s_trt_depot_admission
app.{{app.table.optimized.silver.admission}}.database={{app.database.optimized}}
app.{{app.table.optimized.silver.admission}}.hdfs.path={{app.hdfs.path.optimized}}/{{app.table.optimized.silver.admission}}
app.{{app.table.optimized.silver.admission}}.format=parquet
app.{{app.table.optimized.silver.admission}}.hql.create=${INSTALL}/hql/create_admission.hql
app.{{app.table.optimized.silver.admission}}.safe.hdfs.path={{app.hdfs.path.safe}}/{{app.table.safe.bronze.admission}}

# Alimentation TABLE SAFE ajout
hiv.{{app.table.safe.bronze.ajout}}.hql=${INSTALL}/hql/PAGDATAIA-41/alim_safe_ajout_empile.hql
hiv.{{app.table.safe.bronze.ajout}}.hiveconf.yarn.name={{app.yarn.name.prefixe}}_{{app.table.safe.bronze.ajout}}_alim
hiv.{{app.table.safe.bronze.ajout}}.hivevar.database_safe={{app.database.safe}}
hiv.{{app.table.safe.bronze.ajout}}.hivevar.table_safe_ajout={{app.table.safe.bronze.ajout}}
hiv.{{app.table.safe.bronze.ajout}}.hivevar.database_raw={{app.database.raw}}
hiv.{{app.table.safe.bronze.ajout}}.hivevar.table_raw_ajout={{app.table.raw.ajout}}


# Alimentation TABLE SAFE admission en ajoutant l'historique
app.table.safe.bronze.complet=b_${GLB_MODULE}_admission
hiv.{{app.table.safe.bronze.complet}}.hql=${INSTALL}/hql/PAGDATAIA-41/alim_safe_complet_empile.hql
hiv.{{app.table.safe.bronze.complet}}.hiveconf.yarn.name={{app.yarn.name.prefixe}}_{{app.table.safe.bronze.complet}}_alim
hiv.{{app.table.safe.bronze.complet}}.hivevar.database_safe={{app.database.safe}}
hiv.{{app.table.safe.bronze.complet}}.hivevar.table_safe_ajout={{app.table.safe.bronze.ajout}}
hiv.{{app.table.safe.bronze.complet}}.hivevar.table_safe_admission={{app.table.safe.bronze.admission}}

#********************#
#        HCP         #
#********************#
# Copie HDFS du Safe vers Optimized
hcp.{{app.table.optimized.silver.admission}}.source.table={{app.table.safe.bronze.admission}}



#################  spk.  #####################################
# cles obligatoires pour piloter le lancement du binaire spark
# doivent commencer par "spk."
##############################################################

# binaire a lancer
spk.common.bin=${BDL_SPARK_CMD}

# nom qui doit apparaitre dans YARN
spk.common.name={{app.yarn.name}}

# nom du pool de ressources a utiliser
spk.common.queue={{app.yarn.queue}}

# type de deploiement
spk.common.master=yarn
spk.common.deploy-mode=cluster

# informations pour kerberos
spk.common.keytab=${KRB_USER_KEYTAB}
spk.common.principal=${KRB_USER_PRINCIPAL}

# parametrage du driver et des executors
spk.common.driver-cores=2
spk.common.driver-memory=4g
spk.common.conf.spark.driver.memoryOverhead=2048

spk.common.executor-cores=2
spk.common.executor-memory=6g
spk.common.conf.spark.executor.memoryOverhead=2048

# autre conf (libre, se referer a la doc Spark)
spk.common.conf.spark.logConf=true
spk.common.conf.spark.dynamicAllocation.enabled=true
spk.common.conf.spark.dynamicAllocation.initialExecutors=1
spk.common.conf.spark.dynamicAllocation.minExecutors=1
spk.common.conf.spark.dynamicAllocation.maxExecutors=2
spk.common.conf.spark.hadoop.fs.hdfs.impl.disable.cache=true
# https://spark.apache.org/docs/2.2.0/sql-programming-guide.html#upgrading-from-spark-sql-21-to-22
spk.common.conf.spark.sql.hive.caseSensitiveInferenceMode=NEVER_INFER

# fichiers a envoyer dans HDFS (au moins le fichier de conf log4j)
spk.common.files.01=${PROPERTIES}/{{app.log4j.file}}

# complement au classpath (au moins ceux ci-dessous)
spk.common.driver_executor.extraClassPath.01=.
spk.common.driver_executor.extraClassPath.02=${PROPERTIES}

# options java a transmettre au driver (au moins celles ci-dessous)
spk.common.driver.extraJavaOptions.log4j.configuration={{app.log4j.file}}
spk.common.driver.extraJavaOptions.sun.security.krb5.debug=false

# options java a transmettre aux executors (au moins celles ci-dessous)
spk.common.executor.extraJavaOptions.log4j.configuration={{app.log4j.file}}
spk.common.executor.extraJavaOptions.sun.security.krb5.debug=false


# parametrage HIVE dans un contexte SPARK
spk.common.conf.spark.hadoop.hive.metastore.connect.retries=3600
spk.common.conf.spark.hadoop.hive.metastore.client.connect.retry.delay=1s
spk.common.conf.spark.hadoop.hive.exec.dynamic.partition=true
spk.common.conf.spark.hadoop.hive.exec.dynamic.partition.mode=nonstrict
spk.common.conf.spark.hadoop.hive.exec.max.dynamic.partitions=2000
spk.common.conf.spark.hadoop.hive.exec.max.dynamic.partitions.pernode=2000

# classe main
spk.spark.alimadmission.class=fr.laposte.bdl.bscc.${GLB_MODULE}.spark.AlimSafeDepotAdmission
spk.spark.alimadmission.name={{app.yarn.name.prefixe}}_alim_safe_bronze_admission

spk.common.conf.spark.yarn.maxAppAttempts=1


#################  suez  #####################################
# cles pour parametrer suezlib
# https://wiki.net.extra.laposte.fr/confluence/display/BGDB/SuezLib
##############################################################
app.suez.topics=vt1_cDepositOrRaw_in_v1
app.suez.environnement=acc

app.suez.datacenter=mar
app.suez.lanceur=streaming
app.suez.offset.strategy=normal
app.suez.conversion=rawOnly
app.suez.table.processed.partition=true
app.suez.table.processed.partition.format=%Y-%m-%d-%H

#parametres d'optimisation des streamings
app.suez.spk.common.executor-cores=3
app.suez.spk.common.executor-memory=6g
app.suez.spk.common.driver-memory=1g
app.suez.spk.common.conf.spark.streaming.kafka.maxRatePerPartition=290
app.suez.spk.common.conf.spark.memory.offHeap.size=2g
app.suez.spk.common.conf.spark.memory.offHeap.enabled=true

app.suez.suez.consumer.duration.sec=60
